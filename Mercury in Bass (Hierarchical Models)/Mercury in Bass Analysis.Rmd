---
title: "Mercury in Bass"
author: "Allison Young"
date: "October 17, 2018"
output: html_document
---

```{r}
#R for mercury in bass data, using multi-level models
#install the package "lme4" to use the lmer command.
install.packages("lme4")
library(lme4)
library(lattice)

bass = read.csv("mercurydata.txt", header = T)
dim(bass)
summary(bass)
```

Anything above 1 considered unsafe. diff between median and mean, so possibly rt skew, maybe log mercury...

```{r}
#### Exploratory data analysis to start thinking about models

#let's see how many stations and the counts in each (top row is label, second row is count)
table(bass$station.2)
```

Quite different sample sizes, and can use hierarchical models rather than grouping of categories that are small.

```{r}
#Station is a factor, so let's make it one in the data
bass$station.2 = factor(bass$station.2)

#let's make river a factor as well.
bass$river = factor(bass$river)

#let's look at mercury vs. weight and vs. length
pairs(mercury~ weight + length, data = bass)
```


bottom left, can see linear relationship with mercury and lenght and weight
strong relationship between lenght and weight, so check multicorrelarity

ultimately use lenght for comparison, because less spread out than weight in sample.


```{r}
#perhaps some fanning out, but not obvious fanning -- we might consider a log transformation of mercury
#similar relationships for mercury vs. length and mercury vs. weight
#check correlations among the predictors to look for colinearity

cor(bass$weight, bass$length)
```

```{r}
#quite correlated!  let's just use one of the predictors.  length looks a stronger predictor.

#look at mercury versus length by river -- not much differences across rivers
xyplot(mercury ~ length | river, data = bass)
```




```{r}
#mercury and length by station. maybe some different slopes, but sample sizes are too small to tell
xyplot(mercury ~ length | station.2, data = bass)
```

really small groupings, so even if a little more interesting, difficult to say much.

this is where heirarchical model comes in to shrink sample towards mean.


```{r}
#we might try linear model with mercury and length

#create mean centered length
bass$length.c= bass$length - mean(bass$length)

#try a model using interactions with station
mercreg = lm(mercury ~ length.c * station.2, data = bass)
summary(mercreg)

#diagnostics
plot(y = mercreg$resid, x=bass$length, xlab = "Length", ylab = "Residual")
abline(0,0)
boxplot(mercreg$resid~bass$station.2, xlab = "Station", ylab = "Residual")

#pretty good fit... let's go with it. you could try logs as well.

#see if interaction effects are useful
mercregnoint = lm(mercury ~ length.c + station.2, data = bass)
anova(mercregnoint, mercreg)

#they seem to be useful overall, but many are based on very small sample sizes.
#it would be good to borrow strength across the stations to get better estimates
#that is what a hierarchical model does!
```

Random Effects Models
I Hierarchical models (like school data) can be applied to
regression contexts where observations are grouped
I We discuss models for linear regression (but ideas apply to
logistic regression, Poisson regression, etc.)
I Recall linear model with 2 predictors can be written
yj = Œ≤0 + Œ≤1x1j + Œ≤2x2j + j
, where j ‚àº N(0, œÉ2
)
I Suppose that observations fall in I groups, indexed by i

do same thing for intercepts as means.... shrink intercepts towards average intercept


Random Intercepts Models
Let i index groups and j index observations. The random
intercepts model is
yij = Œ≤0i + Œ≤1x1ij + Œ≤2x2ij + ij
ij ‚àº N(0, œÉ2
i
)
Œ≤0i ‚àº N(Œ≤0, œÑ 2
)
This is also written sometimes as,
yij = Œ≤0 + Œ≤1x1ij + Œ≤2x2ij + Œ≥i + ij
ij ‚àº N(0, œÉ2
i
)
Œ≥i ‚àº N(0, œÑ 2
)
-----------------

Boi -person actual blood pressure (have to be independent)

-------------------

Random Intercepts Models
I Allows separate intercepts for each group, but shrinks
estimates towards common value
I Useful for repeated measurements, when the ‚Äúgroups‚Äù are
individuals (e.g., we take a subject‚Äôs blood pressure three
times and include all three measurements in the model)
I Also useful when some groups have small sample sizes, so that
estimation of intercept is highly variable
I Model implies same slope of x1 for each group and same slope
of x2 for each group

```{r}
#### let's do a hierarchical regression (random effects regression) with lmer


#this call just uses a random intercept
mercreglmerint = lmer(mercury ~ length.c + (1 | station.2), data = bass) # let intercept be different for every station (1|station.2)
summary(mercreglmerint)

#look at the intercepts (and the common slope) for each station
coef(mercreglmerint)

#these equal the fixed effects plus the random effect
fixef(mercreglmerint)
ranef(mercreglmerint)


#this lmer call uses a random intercept and a random slope

mercreglmerintslope = lmer(mercury ~ length.c + ( 1 + length.c  | station.2), data = bass) 
summary(mercreglmerintslope)

#the intercepts and slope for each station
coef(mercreglmerintslope)

#the predicted values of mercury for each bass
preds = fitted(mercreglmerintslope)

#plot residuals versus length
 
plot(y = residuals(mercreglmerintslope), x = bass$length, xlab= "Length", ylab = "Residuals")
```
