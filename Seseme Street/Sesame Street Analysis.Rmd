---
title: "Sesame Street"
author: "Allison Young"
date: "September 25, 2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Setup and Load Analysis Functions
Load libraries and functions I've created to help with basic data analysis and dataset preview
## Analysis Set-Up
### Load Data Libraries, Files (INSERT FILE NAMES HERE), and Functions
```{r, include=FALSE}
#Load libraries
library(readr)
library(ggplot2)
library(lattice)
library(lubridate)
library(openxlsx)


#Working Directory and files
dir <- "C:/Users/ayoung/Desktop/Duke/Git Repositories/Modeling-IDS-702/"  ### INSERT
folder = "Seseme Street"  ### INSERT
fname = "sesame.txt" ### INSERT

#Load Data
sesame <- loadfile(dir,folder,fname)

# Load Functions
source("../Functions/loadfile.R")
source("../Functions/exploredata.r")
source("../Functions/pullfname.R")
source("../Functions/textsummary.R")
```

### Printed Summary of File
```{r}
textsummary(fname,sesame)
```

############# (Ctrl+F and Replace file names for rest of analysis) #################

### Explore Data 
```{r}
exploredata(sesame)
```

### Variable Notes

```{r}
#### Categorized properly? Continuous vs Categorical-- may need to factorize
## Site, Setting

#### Clear outliers? Mins and Max--- leverage? or do we need to exclude from model
## Look at Age, someone with a strong letters score, wide ranges on numbers as well

#### Science considerations?
## pretest may do exponentially better than low score with exposure, keep an eye on prescore

#### Outcome Variable?
## post test number score-- probably can use a normal distribution (focusing on numbers for this analysis)
# Ex. post num = .....+Bprenum  (more flexible bc can estimate a coefficient rather than force it to be 1, as the change value variable need)

## Could also use numbers (change in score), just a slightly different evaluation
# Ex. postnum = 1 x prenumm + stuff
```

### Data Cleaning
Not really needed, re-look at regardkng labels

```{r}
#### Formula to drop columns 
#dropcol(colhead)

#### Formula to drop rows based on value/s 
#droprow(colhead,value)

```


### Initial look at relationships
```{r}
### Add any summaries of interest
##### Formula to create a dictionary that can use formula to compare y and x
#### Formula to compare y to each x
#plot(outcome~predictorA + predictorB + predictorC, data=ref)


plot(y = sesame$postnum, x = sesame$prenum, xlab = "Pretest Numbers", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$age, xlab = "Age", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$prelet, xlab = "Pretest Letters", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$peabody, xlab = "Peabody Score", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$prebody, xlab = "Pretest Body", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$preform, xlab = "Pretest Form", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$preclasf, xlab = "Pretest Classification", ylab = "Posttest numbers")
plot(y = sesame$postnum, x = sesame$prerelat, xlab = "Pretest Relational", ylab = "Posttest numbers")

boxplot(postnumb ~ viewenc, data = sesame, xlab = "Encouraged", ylab = "Posttest Numbers")
boxplot(postnumb ~ site, data = sesame, xlab = "Site", ylab = "Posttest Numbers")
boxplot(postnumb ~ sex, data = sesame, xlab = "Sex", ylab = "Posttest Numbers")
boxplot(postnumb ~ setting, data = sesame, xlab = "Setting", ylab = "Posttest Numbers")
boxplot(postnumb ~ viewcat, data = sesame, xlab = "Viewing Category", ylab = "Posttest Numbers")

```


Findings:

Relationships aren't really strong in XY plots. Possibly something in letters happening.... and a few outliers on numbers (pretest vs posttest)

Clearly site is meaningful. Recognize clearly correlation with viewing and 

```{r}

#NOTE:  viewcat formally is an outcome variable and so should not be considered a predictor, if we are thinking about causal effect of encouragement
#so we won't include it in the analysis.

#let's also see if there are any interactions with site, which is an interesting variable from a policy perspective

xyplot(postnumb ~ prenumb | as.factor(site), data = sesame)
xyplot(postnumb ~ age | as.factor(site),data = sesame)
bwplot(postnumb ~ as.factor(viewenc) | as.factor(site), data = sesame)

```
Site and encouragement are main variables, so focus on that with these new boxplots
Don't see any real differences across sites, 


###Multicoliarity
Looking at if two things say same thing(looking for high numbers between dif variables)
```{r}

#take a look at correlations among predictors for multicollinearity
round(cor(sesame), 3)
```

.718, not immediately sure strong ennough to be an issue (def at .9)
see if standard errors are inflated if using both, can come back and remove one, and then re-do to see
```{r}

#there are a lot of high correlations among the predictors.  Maybe we don't need to include all of the pre-tests to get reasonable models.

#let's center all the continuous predictors
sesame$cprenumb = sesame$prenumb - mean(sesame$prenumb)
sesame$cprelet = sesame$prelet - mean(sesame$prelet)
sesame$cpreform = sesame$preform - mean(sesame$preform)
sesame$cpreclasf = sesame$preclasf - mean(sesame$preclasf)
sesame$cprerelat = sesame$prerelat - mean(sesame$prerelat)
sesame$cprebody = sesame$prebody - mean(sesame$prebody)
sesame$cpeabody = sesame$peabody - mean(sesame$peabody)
sesame$cage = sesame$age - mean(sesame$age)
```

### First Model
```{r}

#let's make a new dummy variable with encouragement = 1 and no encouragement = 0, since we want to interpret results as the effect of encouragement.
sesame$encouraged = rep(0, nrow(sesame))
sesame$encouraged[sesame$viewenc==1] = 1

#do a quick check to make sure the code did what we wanted it to do.
head(sesame)
```

```{r}
## first pass model

reg1 = lm(postnumb ~ as.factor(site) + encouraged + as.factor(sex) + as.factor(setting) + cprenumb + cprelet + cprebody + cpreclasf + cpreform + cprerelat + cpeabody + cage, data = sesame)
 
summary(reg1)
```
Mostly care about encouraged .  

Looks 1.55 (1.55 point higher)-- not really enough to reject possibility not predictive

```{r}
#there are a lot of high correlations among the predictors.  Maybe we don't need to include all of the pre-tests to get reasonable models.

#let's center all the continuous predictors
sesame$cprenumb = sesame$prenumb - mean(sesame$prenumb)
sesame$cprelet = sesame$prelet - mean(sesame$prelet)
sesame$cpreform = sesame$preform - mean(sesame$preform)
sesame$cpreclasf = sesame$preclasf - mean(sesame$preclasf)
sesame$cprerelat = sesame$prerelat - mean(sesame$prerelat)
sesame$cprebody = sesame$prebody - mean(sesame$prebody)
sesame$cpeabody = sesame$peabody - mean(sesame$peabody)
sesame$cage = sesame$age - mean(sesame$age)
```



```{r}
#looks like effects of multicollinearity could be affecting some estimates.
#we could try to drop some of the predictors and see what happens to our story.  but let's look at the residuals first.

plot(y = reg1$resid, x = sesame$cprenum, xlab = "Pretest Numbers", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cage, xlab = "Age", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cprelet, xlab = "Pretest Letters", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cpeabody, xlab = "Peabody Score", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cprebody, xlab = "Pretest Body", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cpreform, xlab = "Pretest Form", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cpreclasf, xlab = "Pretest Classification", ylab = "Residual")
abline(0,0)
plot(y = reg1$resid, x = sesame$cprerelat, xlab = "Pretest Relational", ylab = "Residual")
abline(0,0)
```

These feel really nice!
```{r}
#now with categorical predictors
boxplot(reg1$resid ~ sesame$encouraged, xlab = "Encouraged", ylab = "Residual")
boxplot(reg1$resid ~ sesame$site,  xlab = "Site", ylab = "Residual")
boxplot(reg1$resid ~ sesame$sex, xlab = "Sex", ylab = "Residual")
boxplot(reg1$resid ~ sesame$setting, xlab = "Setting", ylab = "Residual")

#looking pretty good. let's check case influence diagnostics.
```

Pretty perfect! non constant variance of sites was pretty mucht taken care of
encourage is fine
this is a good fitting model

---now the flower on the cake---

```{r}

lev = hatvalues(reg1)
cooks = cooks.distance(reg1)

plot(lev, ylab = "Leverage value")
plot(cooks, ylab = "Cooks Distance")

#no one appears too far out there on leverage. let's take a look at the person with the largest Cooks Distance
sesame[cooks > .075,]

#this it the 50th person in the dataset.  nothing jumps out about this person as odd.... look at leverage again. 
lev[50]

#not particularly big...  no real good reason to exclude this point from a scientific perspective.  let's not do anything about the person for now, since 
#we might reduce predictors anyways!

```

highest cooks distance point is the kid that did great on pre-test and bombed post test-- nothing odd about the person to explude from the model, but keep it in there unless figure out it was a data entry issue like an incomplete test or something


### Nested F tests to look at Variables we think may have 
```{r}

#let's do a nested F test to see if site is a useful predictor
summary(reg1)
reg1nosite = lm(postnumb ~ encouraged + as.factor(sex) + as.factor(setting) + cprenumb + cprelet + cprebody + cpreclasf + cpreform + cprerelat + cpeabody + cage, data = sesame)
 
anova(reg1nosite, reg1)

#it appears to be a useful predictor.
```
```{r}

#let's try an interaction between site and encouragement, since that will address our question of whether encouragement helped disadvantaged students catch up.

reg2 = lm(postnumb ~ as.factor(site) * encouraged + as.factor(sex) + as.factor(setting) + cprenumb + cprelet + cprebody + cpreclasf + cpreform + cprerelat + cpeabody + cage, data = sesame)

anova(reg1, reg2)

#no significant evidence that effects of encouragement differed across sites (types of people)
#so, we can interpret the CIs of the model without interactions.

confint(reg1)
```

### Interpretation

No real male female differences
Talk about how site makes a difference
Also talk about how pretest values may make a difference


encouragement to watch Sesame Street is associated with an increase of aronud 1.6 points (95% CI: -1.1 to 4.2) on the numbers post-test.  #Because this is a wide confidence interval that ranges anywhere from a 1.1 point decrease to a 4.2 point increase, this study does not provide conclusive evidence that encouragement to watch Sesame Street made a difference.

Here we might want to do an analysis of whether encouragement to watch Sesame Street actually made a difference in whether the children actually watch Sesame Street!  That is another outcome variable and so deserves its own regression.

as a sensitivity analysis, we can try dropping some of the pretest scores.
For example, drop prerelat or preclassif since they are somewhat highly correlated with prenumb
It does not appear to make much difference in our overall conclusions about encouragement.  Thus, we might as well keep all of them to aid interpretation.



#### Which predictors should be in my model?
-This is a very hard question and one of intense
statistical research.
-Different people have different opinions on how to
answer the question.
-I will talk about the key issues rather than specific
methods.
-See Sleuth for details on some specific methods.

#### What variables to include?
Depends on the goal of the analysis.

###### Goal: prediction
-Include only variables that are strong predictors of the
outcome.
-Excluding irrelevant variables can reduce the widths of
the prediction intervals.

##### Goal: interpretation and association
-Include all variables that you thought a priori were
related to the outcome of interest, even if they are not
statistically significant (this is Jerry's general recommendation)
-Improves interpretation of coefficients of interest.

#### Common strategies

##### Backward selection
-Fit a large model with all variables of interest
-Drop variables that are deemed irrelevant according to
some criterion. Common ones include
--Drop if p-value > .10 (possibly all at once)
--Drop one, if any, that leads to smallest value of the AIC or BIC
(see page 356 in Sleuth)

##### Forward selection 
-Include variables one-by-one according to some
criterion. Common ones include
-Include one, if any, that leads to the smallest value of the AIC
or BIC.

##### Stepwise selection
-Potentially do one forward step to enter a variable in the
model, using some criterion to decide if it is worth
entering the variable.
-From the current model, potentially do one backwards
step, using some criterion to decide if it is worth
dropping one of the variables in the model.
-Repeat these steps until the model does not change.

##### Penalized regression (lasso, elastic net)
-Place a penalty on the likelihood function that favors
setting small coefficients to zero

Useful in things like genomic data- where tons of variables.

#### Challenges in model selection
-Using an automated procedure might inadvertently lead
you to miss key transformations or interaction effects, since
you will be tempted to press a button.
-May find scientifically vapid models, since relying on
automation rather than science to guide model selection.
-May be many models with very similar values of the
criterion you use in model selection.
-Very difficult to interpret standard errors, because really
you should account for the randomness in the model
selection procedure too.
-If a predictor is potentially important for interpretation,
why not let the data estimate its coefficient rather than set
it to zero? 

Really important to have relationships with subject area experts who understand the field and the data.

#### Strategy for interactions (really hard)
-Use scientific understanding and interest to guide
search for interaction effects
-Remember, interactions are not necessarily warranted
because two predictors are strongly associated
-They are warranted because the relationship of the
outcome with one predictor depends on the value of
another predictor. That has nothing to do with
associations among the predictors themselves.
-You want plausible scientific explanations for the
interactions.
-Otherwise the model is not likely to generalize.

-Risky to use a “throw everything in and then see what we
can keep” approach.
-Could overfit the data (too specific to that data).
-See concerns with stepwise approach.
-Be wary of interactions with categorical variables when you
have few data points in some combinations of levels.
-Including them could result in a model that overfits data.
-If a parsimonious model seems to describe the data
reasonably well, then it can be useful for interpretations
and generalizability to use parsimonious model.




NOTE : if don't know what to make baseline, pick the one with the largest sample