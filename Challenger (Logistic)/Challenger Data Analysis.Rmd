---
title: "Challenger Data Analysis"
author: "Allison Young"
date: "September 27, 2018"
output: html_document
---

Regression is great when your outcome variable is continuous.

With a binary outcome, scatterplots are not very useful tools. So want to look at Boxplots.


```{r}
#install.packages("arm")
library(arm)

library(readr)
challenger <- read_csv("challenger.csv")
View(challenger)
```

#### Logistic Regression
-Often we want to predict or explain a binary outcome variable (e.g., healthy or not, employed or not, win or lose) from several predictors.
-Linear regression not appropriate, because normality makes no sense for errors!
-We use logistic regression.
-Relies on odds and odds ratios, which we review to begin.

Based totally on Odds and Odds Ratios (Relative Risk makes more sense, but OR are more closely tied to Logistic regression)

More appropriate distribution of Y
 Assume for any observation i that where is some function of
 What should function look like?
 Linear: . can be outside (0, 1)!
 Log-linear: . can be > 1!
(see formmulas in notes)

```{r}

#Analysis of the space shuttle Challenger data (see course slides for details)

challenger = read.csv("challenger.csv", header = T)

#change names to lower case
names(challenger)=c("temp", "failure")

#create indicator variable: zero if no failure and 1 if failure
challenger$fail01 = rep(0, 24)
challenger$fail01[challenger$failure=="Yes"] = 1

#let's see what a plot of temperature vs. fail01 looks like
plot(challenger$fail01, x = challenger$temp, xlab = "Temperature", ylab = "Failure")
```

```{r}
#this is not easy to interpret.  We can try a box plot...

boxplot(challenger$temp~challenger$fail01, ylab = "Temperature", xlab = "Failure")
```

MUST USE GLM AND SET FAMILY
```{r}
#this doesn't really portray a prediction of failure from temperature, but it's somewhat useful to get an overall sense of the data.

#let's mean-center the temperature
challenger$tempcent = challenger$temp - mean(challenger$temp)

#let's fit a logistic regression to the mean centered temp
logreg = glm(fail01 ~ tempcent, family = binomial, data= challenger) 

#MAKE SURE YOU USE THE glm COMMAND, NOT THE lm COMMAND.
#AND MAKE SURE TO INCLUDE THE family = binomial.

#look at results
summary(logreg)
```

No more R Squared, that is a linear model. Can interpret using p values and confidence intervals

```{r}
#confidence intervals for the coefficients
confint.default(logreg)
exp(confint.default(logreg))
exp(-.17)
exp(.17)
```
if increase by 1 degree in temperature, expect odds of at leeast one O ring failure to increase by a multiplicitive factor of 0.8436648

to be easier, would to make it negative rather tha positive-- - 1 rather than plus one in equation...

Then, if decrease by 1 degree in temperature, expect odds of at least one O ring failure to increase by 18.53%
```{r}
exp(-1.1)
```

Odds of having at least 1 o ring failure if temperature was average of dataset is .3329  (see above, model intercept of -1.1 )

equates to about 1 to 2 relationship


#### Interpreting Coefficients (see slides)
-As we increase x by 1 unit, we increase the log-odds
for Y by
-Equivalently, we increase the odds for Y by
-With mean-centering of X, is the log-odds for Y
at the mean of X.
-Equivalently, with mean-centering of X,is the odds for Y at the mean of X.
-Often much easier to interpret results by graphing the (predicted) probabilities for values of X.

change in log odds means squish logs together and get.....something....odds ratio....review math to totally understand 
graph predicted probabilities as a function of X (if odds ratio is difficult to interpret) see how predicted probabilities change to interpret log function

##### Estimation of Coefficients
-Use maximum likelihood estimation.
-Basic idea is to find the values of that make the observed values of Y most likely to have
arisen.
-Requires multivariable calculus and numerical methods (Newton Raphson algorithm) for
estimation.
-R does if for us!!! 

#### Intervals and Significance Tests
-As with all coefficients, the standard errors represent chance deviations in the estimated values
from the actual values .
-Confidence intervals based on large-sample normal distribution approximations.
-For significance tests, I recommend the change in deviance test, which we will discuss next time.

```{r}
#here is how to get the predicted probabilities for the observed cases
predprobs = predict(logreg, type = "response")

#useful to examine a plot of predicted probabilities by X
plot(y=predprobs, x = challenger$tempcent, xlab = "Temperature (Centered)", ylab = "Predicted Probability of Failure")

#can show on original scale, too
plot(y=predprobs, x = challenger$temp, xlab = "Temperature", ylab = "Predicted Probability of Failure")
```
type = "response" is important

This interpretation is much more easy to understand than describing the odds.


```{r}
#you might want to make a graph with more values of temperature. You have to predict new observations -- more on that later.

#predicted probabilities at new temperatures, say 36 degrees and 68 degrees

newdata = challenger[1:2,]
newdata$temp[1] = 36
newdata$tempcent[1] = 36 - mean(challenger$temp)
newdata$temp[2] = 68
newdata$tempcent[2] = 68 - mean(challenger$temp)

predict(logreg, newdata, type="response", se.fit=T)

#you can get 95% prediction intervals for the probabilities by using 1.96 as a multiplier
#for 68 degrees, the 95% prediction interval for the probability of failure is (.315 - 1.96*.1118, .315 + 1.96*.1118) 
```

mean center temps and put in new data with the predictions you want.

99% chance of failure at 36 degrees
gives you standard error, so have to create intervals yourself for predictions
So....interval goes beyond 1--- just cap at 1 for interpretation
This is because a large extrpoloation of data (sample didn't include these points (36))

```{r}
### DIAGNOSTICS  #####

#let's look at raw residuals

rawresids = challenger$fail01 - predprobs

plot(y=rawresids, x=challenger$tempcent, xlab = "Temperature (centered)", ylab = "Residuals")

#raw residuals are not very useful!!  Can look to see which cases have values near 1 and -1 to look for
#cases that don't fit well, but not too useful otherwise 

###binned residuals -- used like residual plots in linear regression.

#note: binned plots don't work so well on small sample sizes, like these data. So, the code below 
#is mostly intended for the syntax.  the script "Interpreting binned plots" shows a better example.
#Note: in class we switched to that script at this point.
```

#### Checking the fit of model
-Residuals do not work well. (values are 0 and 1)
-They are always positive when Y=1 and always negative when Y=0.
-Constant variance not assumption of logistic regression.
-No normality of residuals either
-Check if function of predictors well specified
-Binned residuals
-Confusion matrix and ROC curves



############ See Bin Notes Below before returning to this part of the Challenger proble#######################

```{r}
#install the arm package in R to use the binnedplot command.
#install.packages("arm")
#library(arm)

#pick number of classes so you have a decent sample size in each class. 
#you can let the binnedplot command pick the number of classes, since it has sensible defaults

#plot versus predicted probabilities. useful as a "one-stop shopping" plot. 
# useful when many X variables and you want an initial look at model adequacy

binnedplot(x= predprobs, y= rawresids, xlab = "Predicted Probabilities")

#also can plot versus individual predictors
binnedplot(x= challenger$tempcent, y= rawresids,  xlab = "Binned Temperatures (centered)")
```
Sample too small to really use binned residual plot with this dataset


```{r}
#there are so few data points here that it is hard to judge the quality of the plots.
#really you want at least 100 data points before these plots start being useful...

#note: you can use the binnedplot command for exploratory data analysis, too!  Just input the outcome variable for y = ..., and the predictor variable for x = ....
#we will see this in the analysis in the script, "logistic regression 2"
#this command is only useful for continuous predictors -- if you have categorical predictors use the tapply command (see R script for "logistic regression 2")
#when using the plot for exploratory purposes, ignore the SE lines -- they are not valid when using the outcome
```

#### Confusion matrix
-Estimated probabilities can be used to predict outcomes.
-For example, we could decide to predict Y=1 when the predicted probability exceeds 0.5 and predict
Y=0 otherwise.
-We then can determine how many cases we classify
correctly and incorrectly
-Resulting 2 x 2 table called the confusion matrix
-When mis-classification rates are high, model may not be an especially good fit to the data.

High sensitivity and high specificity are  important for a good model

```{r}
### here is how to make a Confusion Matrix
#first select the threshold for the predicted probabilities.
#Above the threshold, you would predict that they are are 1
#you can try any threshold you want -- just change the value of "threshold"

threshold = 0.5
table(challenger$fail01, logreg$fitted > threshold)

#     FALSE TRUE
#  0    16    1
#  1     4    3

#in the output, the 0 row corresponds to true y_i = 0, and the 1 row corresponds to true y_i = 1
#the FALSE column corresponds to predicted probabilities less than the threshold
#the TRUE column corresponds to predicted probabilities above the threshold
#ideally most of the count is on the diagonal, which is what we see here.

#sensitivity -- true positive rate -- at 0.5 threshold
#3 / (3 + 4)

#specificity -- true negative rate -- at 0.5 threshold
16 / (16 + 1)

#1 - specificity is the false positive rate.  1 - 16/17 = 1/17
```

This code will give you a confusion matiric, predictive probablilities and threshhold you are interested in 

16 and 3 are good, 4 is a little off
Will change with different thresholds 

Sensitivity: 3/(3+4)
Specificity: 16/(16+1)

##### Sensitivity and Specificity
Sensitivity: among all cases with y=1, the fraction with
predicted probability above threshold
-Also called the true positive rate

Specificity: among all cases with y=0, the fraction with
predicted probability below threshold 1 – specificity is called the false positive rate
-Want high values of sensitivity and low values of (1 –
specificity)


#### ROC Curves
-Receiver operating characteristic (ROC) curve plots
-Sensitivity on Y axis
-1 – specificity on X axis
-Evaluated at lots of different values for the threshold
-Good fitting logistic regression curves toward the
upper left corner, with area under the curve (AUC)
near one.
-Make ROC curves in R using the pROC package. See R
script.

```{r}
##ROC curve -- plots sensitivity vs 1 - specificity for an expansive set of thresholds
# first install the pROC package
#install.packages("pROC")
library("pROC")

roc(challenger$fail01, fitted(logreg), plot=T, legacy.axes=T)

#can add the "best" threshold to the graph (one with highest sum of sensitivity and specificity) 

roc(challenger$fail01, fitted(logreg), plot=T, print.thres="best", legacy.axes=T)

```

ROC curves are best for comparing models (others are good at seeing something wrong with model, but this is better to determine best) - bigger AUC one you prefer (area under the curve)


### Exploration of binned residual plots for logistic regression

#### Binned residuals
-Compute raw residuals for fitted logistic regression.
-Order observations by values of predicted probabilities from the fitted regression, or by the predictor itself.
-Using ordered data, form g bins of (approximately) equal size. Default: g = square root of sample size.
-Compute average residual in each bin.
-Plot average residual versus average predicted probability (or average predictor value) for each bin.
-Done with “arm” package in R

Look for patterns suggesting model improvements
-Nonlinear trend: suggest include squared term for predictor or possibly use log of predictor
-Bins with large average residuals (in absolute value) in multiple logistic regression: Examine
averages of other predictors across the bins. If large residuals correspond to particular
combinations of variables, may need interaction 

```{r}
# we use simulated datasets to see what binned residual plots can reveal about the fit
# of a logistic regression

#let's make an arbitrary predictor, say X, generated to be always positive
#use a uniform distribution on 1 to 10, taking n = 500 random draws.

n = 5000
X = runif(n, 1, 10)

#now let's make a logistic regression function

beta0 = -3
beta1 = 0.5
p = exp(beta0 + beta1*X)/(1 + exp(beta0 + beta1*X))

#take a look at values of p
summary(p)
```
Low about 8%, high about 88%

use -3 to  make sure probabilities were kinda small.....as an example

```{r}
#now generate values of Y from Bernoulli distributions with probabilities in p

Y = rbinom(n, 1, p)

#look at Y
mean(Y)

goodfit = glm(Y~X, family = "binomial")

summary(goodfit)
```

true value was -3 and intercept was .5 ---- look summary is really close with my model! we were able to create a model and see a good fitting logistical regression

below:
make predicted probabilites
make raw residual values (y- predicted probability)
then create bin plot

```{r}
predprobsgoodfit = predict(goodfit, type = "response")
rawresidsgoodfit = Y - predprobsgoodfit

binnedplot(x= predprobsgoodfit, y= rawresidsgoodfit,  xlab = "Predicted Probability")

#you see no systematic patterns, suggesting the model describes the data reasonably well
#that's good, since we actually fit the right model!
```
said all the points have a predictive probablity in x range.--- more or less equals the average of y????--- review this-- numbers close to zero

each point represents a "bin"

looking for nice random scatter--- equal number above and below  (comparing predictive probabilities to % of 1s)

lines are +/- 2 standard error bars, so a few outside the line are ok, but a ton may need to re-visit model.



```{r}
#now let's see what happens if we needed a squared term but forgot to include it.

beta0 = -3
beta1 = 1
beta2 = -.1
p = exp(beta0 + beta1*X + beta2*X^2)/(1 + exp(beta0 + beta1*X + beta2*X^2))

#take a look at values of p
summary(p)
```


```{r}
#now generate values of Y from Bernoulli distributions with probabilities in p

Y = rbinom(n, 1, p)

#look at Y
mean(Y)

badfit = glm(Y~X, family = "binomial")

summary(badfit)
```
## summary if ignore quadratic term--- nowhere near actual values of x and intercept

```{r}
predprobsbadfit = predict(badfit, type = "response")
rawresidsbadfit = Y - predprobsbadfit

binnedplot(x= predprobsbadfit, y= rawresidsbadfit,  xlab = "Predicted Probability")
```
Clearly really BAD.

Way off at low predicted probabilities, in middle, and at end--- see that quadratic trend in residuals is suggestive of quadratic trend in the model.

```{r}
#predictions are really bad for large values of X.  We have a lot of negative residuals,
#suggesting that the predicted probabilities are way too high.

#what happens when we add the quadratic term?
 
Xsq = X^2
goodfitquad = glm(Y ~ X + Xsq, family = "binomial")

summary(goodfitquad)

predprobsgoodfitquad = predict(goodfitquad, type = "response")
rawresidsgoodfitquad = Y - predprobsgoodfitquad

binnedplot(x= predprobsgoodfitquad, y= rawresidsgoodfitquad,  xlab = "Predicted Probability")
```

Here put in the "right" model. fit with x and x^2 --- intercept and x closer to real numbers, much better

Much better binned residual plot....still off at x values--- check for data errors, or something--- with this particular data set not much else you can do because only one x value.



```{r}

#one more example -- let's see what happens when we need to use log(X) as the predictor

beta0 = -3
beta1 = 2
p = exp(beta0 + beta1*log(X))/(1 + exp(beta0 + beta1*log(X)))

#take a look at values of p
summary(p)

#now generate values of Y from Bernoulli distributions with probabilities in p

Y = rbinom(n, 1, p)

#look at Y
mean(Y)

badfit = glm(Y~X, family = "binomial")

summary(badfit)
```

log of x as predictor


```{r}
predprobsbadfit = predict(badfit, type = "response")
rawresidsbadfit = Y - predprobsbadfit

binnedplot(x= predprobsbadfit, y= rawresidsbadfit,  xlab = "Simulated X value")

#predictions are really bad for large values of X.  We have a lot of negative residuals,
#suggesting that the predicted probabilities are way too high.

#what happens when we use log(X)?
 
logX = log(X)
goodfitlog = glm(Y ~ logX, family = "binomial")

summary(goodfitlog)

predprobsgoodfitlog = predict(goodfitlog, type = "response")
rawresidsgoodfitlog = Y - predprobsgoodfitlog

binnedplot(x= predprobsgoodfitlog, y= rawresidsgoodfitlog,  xlab = "Simulated X value")

```

Without the log, can see estimates that look nothing like what had before, and can see the pattern again (similar to x2).
x +x^2 and log actually look pretty similar , and hard to determine which is better--- asemptope (log) or will it come back down (quadradic )
 
 
 REally difficult to interpret without bin plots! These are really important for knowing if your  model fits or not.Need good size data sets for bins to be useful (otherwise chopping into small groups)
 
### ROC curve 
```{r}
roc(Y, fitted(goodfitlog), plot=T, print.thres="best", legacy.axes=T)
```
